{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a TrajCast Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a short guide to training a *TrajCast* model for paracetamol. Usually, this involves the following steps:\n",
    "- Generate a dataset from raw MD trajectory data (not covered here).\n",
    "- Prepare and choose the appropriate hyperparameters in a configuration file.\n",
    "- Train the *TrajCast* model.\n",
    "- Track the training progress.\n",
    "\n",
    "> **_Note_:** \n",
    "> This code is still under development. This notebook is intended to provide help and guidance in getting started. Future updates will focus on making the code more efficient and user-friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than generating our dataset from scratch by computing displacement and velocity vectors at a specified time interval, here, we skip this step and download a subset of the dataset used for the experiments of paracetmol in our preprint, available on HuggingFace. This dataset comprises 500 configurations for training, 125 for validation, and 100 for testing, respectively. **These datasets are only 5% of the size used in the experiments in our manuscript.** Of course, feel free to use the datasets used in the original experiments.\n",
    "\n",
    "> **_Note_:** \n",
    "> You can preprocess your own dataset using `compute_additional_fields` within the `ASETrajectory` class. We will add this to the notebook later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "for dataset in [\"train\", \"val\", \"test\"]:\n",
    "    hf_hub_download(\n",
    "        repo_id=\"ibm-research/trajcast.datasets-arxiv2025\",\n",
    "        repo_type=\"dataset\",\n",
    "        revision=\"main\",\n",
    "        filename=f\"example/{dataset}.extxyz\",\n",
    "        local_dir=\"../data\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Configuration Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data at the right place, we can now start preparing for training. For this we create configuration dictionary with all necessary hyperparameters and settings. *TrajCast* arranges this dictionary based on three different categories: model, data, and training.\n",
    "\n",
    "> **_Note_:** \n",
    "> Here, we build a **small model** that is likely not very accurate, allowing for training on a CPU, as this notebook is primarily intended to help users get started with the code. However, feel free to adjust the parameters to match those in the paper for more accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"precision\": 32,  # Floating point precision, 32 for single, 64 for double\n",
    "    \"num_chem_elements\": 4,  # Number of different chemical species the model means to describe\n",
    "    \"edge_cutoff\": 4.0,  # Distance cutoff for message passing\n",
    "    \"num_edge_rbf\": 8,  # Number of radial basis functions edge\n",
    "    \"num_edge_poly_cutoff\": 6,  # p of polynomial cutoff function,\n",
    "    \"vel_max\": 0.11,  # Cutoff for velocities obtained from Maxwell-Boltzmann distribution\n",
    "    \"num_vel_rbf\": 8,  # Number of radial basis functions velocities\n",
    "    \"max_rotation_order\": 1,  # Rotation order up to which we will resolve features and spherical harmonics\n",
    "    \"num_hidden_channels\": 16,  # Number of features per irrep\n",
    "    \"num_mp_layers\": 3,  # Number of message passing layers\n",
    "    \"edge_mlp_kwargs\": {  # Settings for MLP producing weights for edge tensor product\n",
    "        \"n_neurons\": [\n",
    "            16,\n",
    "            16,\n",
    "            16,\n",
    "        ],  # Number of neurons per layer, here 3 hidden layers with 16 neurons each\n",
    "        \"activation\": \"silu\",  # Chosen activation function\n",
    "    },\n",
    "    \"vel_mlp_kwargs\": {  # Settings for MLP producing weights for velocity tensor product\n",
    "        \"n_neurons\": [\n",
    "            16,\n",
    "            16,\n",
    "            16,\n",
    "        ],  # Number of neurons per layer, here 3 hidden layers with 16 neurons each\n",
    "        \"activation\": \"silu\",  # Chosen activation function\n",
    "    },\n",
    "    \"nl_gate_kwargs\": {  # Settings for non-linear gates at the end of the update\n",
    "        \"activation_scalars\": {  # Activation functions for scalar features\n",
    "            \"o\": \"tanh\",  # Odd features\n",
    "            \"e\": \"silu\",  # Even features\n",
    "        },\n",
    "        \"activation_gates\": {  # Settings for scalars which will be used to scale L>0 features after non-linearity\n",
    "            \"e\": \"silu\"  # We only use even features for gating\n",
    "        },\n",
    "    },\n",
    "    \"conserve_ang_mom\": True,  # Whether to conserve angular momentum,\n",
    "    \"o3_backend\": \"e3nn\",  # Whether to use e3nn or cueq, we recommend the latter particularly for large systems\n",
    "    \"net_lin_mom\": [\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "    ],  # Net linear momentum you'd expect (usually the 0 vector if momentum was properly zeroed in equilibrium MD)\n",
    "    \"net_ang_mom\": [0.0, 0.0, 0.0],  # Same as above but for angular momentum\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Note_:** \n",
    "> This is the training data only. Currently, validation data will be passed as part of the training dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"root\": \".\",  # Directory where the data lies\n",
    "    \"name\": \"paracetamol_training\",  # Name the processed dataset should have\n",
    "    \"cutoff_radius\": 4.0,  # Cutoff for defining edges between nodes, should be the same as for model, will be fixed later\n",
    "    \"files\": [\n",
    "        \"../data/example/train.extxyz\"\n",
    "    ],  # Files with the data, can be multiple ones\n",
    "    \"rename\": True,  # Dependent on the precision it will add a tag to the processed filename\n",
    "    \"atom_type_mapper\": {  # Mapping chemical atom types to variables within the model, not necessary but less error prone\n",
    "        1: 0,  # H -> 0\n",
    "        6: 1,  # C -> 1\n",
    "        7: 2,  # N -> 2\n",
    "        8: 3,  # O -> 3\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {\n",
    "    \"seed\": 1705,  # Random seed for initialisation\n",
    "    \"model_type\": \"EfficientTrajCastModel\",  # The type of model you want to train, do not change!\n",
    "    \"device\": \"cpu\",  # Device on which to run the training, set to cpu here in case this is run without\n",
    "    \"restart_latest\": False,  # Whether to start from an old checkpoint\n",
    "    \"target_field\": \"target\",  # Name of the field where the model will save its prediction\n",
    "    \"reference_fields\": [  # Where the true labels are saved\n",
    "        \"displacements\",\n",
    "        \"update_velocities\",\n",
    "    ],\n",
    "    \"batch_size\": 10,  # How many configurations are contained in one batch, here 10\n",
    "    \"max_grad_norm\": 0.5,  # Gradient clipping\n",
    "    \"num_epochs\": 10,  # Maximum number of epochs to be performed, usually longer, here we just pick 10\n",
    "    \"criterion\": {  # This is to set up the loss function, will be updated and simplified later\n",
    "        \"loss_type\": {\"main_loss\": \"mse\"},\n",
    "        \"learnable_weights\": False,\n",
    "    },\n",
    "    \"optimizer\": \"adam\",  # Which optimizer to use\n",
    "    \"optimizer_settings\": {  # Settings for chosen optimizer\n",
    "        \"lr\": 0.01,  # Learning rate\n",
    "        \"amsgrad\": True,  # Whether AMSGrad is turned on\n",
    "    },\n",
    "    \"scheduler\": [\"ReduceLROnPlateau\"],  # Which Schedulers to use, can be multiple\n",
    "    \"scheduler_settings\": {  # Settings dictionary for each scheduler\n",
    "        \"ReduceLROnPlateau\": {\"factor\": 0.8, \"patience\": 25, \"min_lr\": 0.0001}\n",
    "    },\n",
    "    \"chained_scheduler_hp\": {  # Interaction between schedulers, this is outdated and will be removed soon\n",
    "        \"milestones\": [\n",
    "            10000000\n",
    "        ],  # For using only 1 scheduler make sure this value is larger than the total number of updates\n",
    "        \"per_epoch\": True,  # Adjust LR per epoch rather than per batch\n",
    "        \"monitor_lr_scheduler\": False,  # For debugging, LR is monitored anyway\n",
    "    },\n",
    "    \"tensorboard_settings\": {  # Validation and tracking of weights, loss, and LR happens in tensorboard\n",
    "        \"loss\": True,  # Track loss\n",
    "        \"lr\": True,  # Track lr\n",
    "        \"loss_validation\": {  # Compute validation loss based on a validation set defined in \"data\"\n",
    "            \"data\": {\n",
    "                \"root\": \".\",  # Directory where the data lies\n",
    "                \"name\": \"paracetamol_validation\",  # Name the processed dataset should have\n",
    "                \"cutoff_radius\": 4.0,  # Cutoff for defining edges between nodes, should be the same as for model and training set, will be fixed later\n",
    "                \"files\": [\n",
    "                    \"../data/example/val.extxyz\"\n",
    "                ],  # Files with the data, can be multiple ones\n",
    "                \"rename\": True,  # Dependent on the precision it will add a tag to the processed filename\n",
    "                \"atom_type_mapper\": {  # Mapping chemical atom types to variables within the model, should be the same as for training set\n",
    "                    1: 0,  # H -> 0\n",
    "                    6: 1,  # C -> 1\n",
    "                    7: 2,  # N -> 2\n",
    "                    8: 3,  # O -> 3\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the dictionaries and build trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trajcast.model.training import Trainer\n",
    "\n",
    "config = {}\n",
    "config[\"model\"] = model_dict\n",
    "config[\"data\"] = data_dict\n",
    "config[\"training\"] = train_dict\n",
    "\n",
    "trainer = Trainer(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once, the `Trainer` class is intialised, we can easily access it's attributes such as `dataset` and `model`. Now, however, we will start training our model for the specified short number of 10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a *TrajCast* model is simple and just requires a call of the `train` routine of the `Trainer` class. This will immediately kick-off the training and will generate directories for logging, checkpointing, and tracking the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the produced directories and files:\n",
    "- `logs`: Logging of all model and training parameters as well as training and validation loss of each epoch.\n",
    "- `checkpoints`: Stores state dictionaries of best model and all relevant information of the latest epoch.\n",
    "- `tb_log`: Tensorboard with validation to track and easily visualize relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now extract and visualize the training and validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = [\"loss_training\", \"loss_validation\"]\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "for metric in metrics:\n",
    "    event = event_accumulator.EventAccumulator(f\"tb_log/{metric}\")\n",
    "    event.Reload()\n",
    "    data = event.Scalars(\"loss\")\n",
    "    epochs = [i.step for i in data]\n",
    "    loss = [i.value for i in data]\n",
    "    ax.plot(epochs, loss, label=metric)\n",
    "\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Note_:** \n",
    "> You can also plot the MAE for displacements and velocities by replacing the paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model training has finished, ideally with converged learning curves, we can deploy our model for inference and forecasting. For more information on this please refer to this [example notebook](../inference/forecasting.ipynb).\n",
    "\n",
    "The last thing to do here, is to save our configuration dictionary to a YAML file so we can load our state dictionaries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.dump_config_to_yaml(\"config_example.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
